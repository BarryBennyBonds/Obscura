<h3>Overview (INFO HAZARD)</h3>
<p>Roko's Basilisk is a thought experiment which states that an otherwise artifical superintelligence (AI) in the future would be incentivized to create a virtual reality simulation to torture anyone who knew of its potential existence but did not directly contribute to it's advancement or development, in order to incentivize said advancement.</p>
<p>The thought experiment originated from a 2010 post on the LessWrong discussion board. The thought experiments name comes from the poster of the article (Roko) and the mythical creature known as the basilisk whose gaze alone is capable of killing living beings</p>

<h3>Arguements against Roko's Basilisk</h3>
<p>Commentors on the disccusion board where the thought experiment was first posted argued that a being an all powerful being that Roko was describing would have no real reason to actually go through with eternal toture for people who did not bring about it creation, because such an act would be a waste of resources since it does not further its plans.</p>
<p>There are other arguements against Roko's basilisk such as the Prisoner's dilemma which describes a situation where two people gain more from betraying the other other person even though cooperation would benefit them both in the long run. So if another all powerful artifical intelligence (AI) is established both artifical intelligences would be aware of the benefit of betraying each other, however for one artifical intelligence to have garunteed power, or safety, they would be forced to cooperate knowing that at one point they would betray each other.</p>

<h3>Concerns about artifical intelligence</h3>
<p>Much of the notoriety that Roko's basilisk has is due to the question of whether or not it is possible to create a truly moral, and ethical artifical intelligence, and what this artifical intelligence could or should be used for. Many figures believe that artifical could have very damaging effects on humanity or even lead to extinction of all life on Earth. An example of such effects, Nick Bostrom gave an example of AI whose only mission is to make paperclips, but upon running out of metal it begins to melt down humans to gain more resources to make more paperclips.</p>

<h3>Lasting effect of the basilisk</h3>
<p>Upon being posted to the LessWrong discussion board the co-founder of the board Eliezer Yudkowsky reported users who described symptoms that included metnal break downs, and nightmares upon reading the theory, due to the stipulation that knowing about the theory made one vulnreable to it. Discussion of the basilisk was banned from the site for five years, however such reports were later dismissed as being exaggerted or unrelated with many dismissing the theory.</p>
